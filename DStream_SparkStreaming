#DstreamSparkJob
package com.kehaque634.spark.jobs.RealTimeStreaming

import com.kehaque634.cassandra.dao.{KafkaOffsetRepository, CreditcardTransactionRepository}
import com.kehaque634.cassandra.{CassandraDriver, CassandraConfig}
import com.kehaque634.config.Config
import com.kehaque634.creditcard.Schema
import com.kehaque634.kafka.KafkaConfig
import com.kehaque634.spark.{DataReader, GracefulShutdown, SparkConfig}
import com.kehaque634.spark.jobs.SparkJob
import com.datastax.spark.connector.cql.CassandraConnector
import org.apache.kafka.clients.consumer.ConsumerConfig
import org.apache.log4j.Logger
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types.{TimestampType, DoubleType, IntegerType}
import org.apache.spark.streaming.{Duration, StreamingContext}
import org.apache.spark.streaming.kafka010.KafkaUtils
import org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent
import org.apache.spark.streaming.kafka010.ConsumerStrategies._


import scala.collection.mutable.Map

/**
 * Created by khondaker on 11/8/20.
 */
object DstreamFraudDetection extends SparkJob("Fraud Detection using Dstream"){

  val logger = Logger.getLogger(getClass.getName)

  def main (args: Array[String]){

    Config.parseArgs(args)

    import sparkSession.implicits._
    

    /*
       Connector Object is created in driver. It is serializable.
       So once the executor get it, they establish the real connection
    */
    val connector = CassandraConnector(sparkSession.sparkContext.getConf)

    val brodcastMap = sparkSession.sparkContext.broadcast(Map("keyspace" -> CassandraConfig.keyspace,
    
      "kafkaOffsetTable" -> CassandraConfig.kafkaOffsetTable))


    val ssc = new StreamingContext(sparkSession.sparkContext, Duration(SparkConfig.batchInterval))


    val topics = Set(KafkaConfig.kafkaParams("topic"))
    val kafkaParams = Map[String, String](
      ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG -> KafkaConfig.kafkaParams("bootstrap.servers"),
      ConsumerConfig.GROUP_ID_CONFIG -> KafkaConfig.kafkaParams("group.id"),
      ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG ->
        "org.apache.kafka.common.serialization.StringDeserializer",
      ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG ->
        "org.apache.kafka.common.serialization.StringDeserializer",
      ConsumerConfig.AUTO_OFFSET_RESET_CONFIG -> KafkaConfig.kafkaParams("auto.offset.reset")
    )


    val storedOffsets = CassandraDriver.readOffset(CassandraConfig.keyspace,
                           CassandraConfig.kafkaOffsetTable, KafkaConfig.kafkaParams("topic"))

    val stream = storedOffsets match {
      case None => {
        KafkaUtils.createDirectStream[String, String](ssc,
                       PreferConsistent,
                       Subscribe[String, String](topics, kafkaParams)
                       )
      }

      case Some(fromOffsets) => {
        KafkaUtils.createDirectStream[String, String](ssc,
                       PreferConsistent,
                       Assign[String, String](fromOffsets.keys.toList, kafkaParams, fromOffsets))
      }
    }

    val transactionStream =  stream.map(cr => (cr.value(), cr.partition(), cr.offset()))
	
	

transactionStream.foreachRDD(rdd => {

      if (!rdd.isEmpty()) {

        val kafkaTransactionDF = rdd.toDF("transaction", "partition", "offset")
          .withColumn(Schema.kafkaTransactionStructureName, // nested structure with our json
            from_json($"transaction", Schema.kafkaTransactionSchema)) //From binary to JSON object
          .select("transaction.*", "partition", "offset")
          .withColumn("amt", lit($"amt") cast (DoubleType))
          .withColumn("merch_lat", lit($"merch_lat") cast (DoubleType))
          .withColumn("merch_long", lit($"merch_long") cast (DoubleType))
          .withColumn("trans_time", lit($"trans_time") cast (TimestampType))


val words = ... // streaming DataFrame of schema { timestamp: Timestamp, word: String }

// Group the data by window and word and compute the count of each group

val windowedCounts = transactionStream.groupBy(
  window($"trans_time", "3 minutes", "2 minutes"),
  $"transactionStream"
).count()
       
        
          
          val keyspace = brodcastMap.value("keyspace")
          val kafkaOffsetTable = brodcastMap.value("kafkaOffsetTable")


          connector.withSessionDo(session => {
           
            val preparedStatementOffset = session.prepare(KafkaOffsetRepository.cqlOffsetPrepare(keyspace, kafkaOffsetTable))

            val partitionOffset:Map[Int, Long] = Map.empty
           
              //Get max offset in the current match
              val kafkaPartition = record.getAs[Int]("partition")
              val offset = record.getAs[Long]("offset")
              partitionOffset.get(kafkaPartition) match  {
                case None => partitionOffset.put(kafkaPartition, offset)
                case Some(currentMaxOffset) => {
                  if(offset > currentMaxOffset)
                    partitionOffset.update(kafkaPartition, offset)
                }

              }

            })
            partitionOffset.foreach(t => {
              // Bind and execute prepared statement for Offset Table
              session.execute(KafkaOffsetRepository.cqlOffsetBind(preparedStatementOffset, t))

            })

          })
        })


      }
      else {
        logger.info("Did not receive any data")
      }

    })

    ssc.start()
    GracefulShutdown.handleGracefulShutdown(1000, ssc)
  }

}
